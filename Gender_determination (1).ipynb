{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gender determination.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J0xBTfntYvv"
      },
      "source": [
        "# Gender Determination by Morphometry of Eyes\n",
        "## Problem Statement\n",
        "The anthropometric analysis of the human face is an essential study for performing craniofacial plastic and reconstructive surgeries. Facial anthropometrics are affected by various factors such as age, gender, ethnicity, socioeconomic status, environment, and region.  \n",
        "\n",
        " \n",
        "\n",
        "Plastic surgeons who undertake the repair and reconstruction of facial deformities find the anatomical dimensions of the facial structures useful for their surgeries. These dimensions are a result of the Physical or Facial appearance of an individual. Along with factors like culture, personality, ethnic background, age; eye appearance and symmetry contributes majorly to the facial appearance or aesthetics. \n",
        "\n",
        " \n",
        "\n",
        "Our objective is to build a model to scan the image of an eye of a patient and find if the gender of the patient is male or female."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zveMLP6FtS4R"
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1f7uslI-ZHidriQFZR966_aILjlkgDN76',\n",
        "dest_path='content/eye_gender_data.zip',\n",
        "unzip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmcjywVHzpKg"
      },
      "source": [
        "# Loading libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60zSKOWGtZ_h"
      },
      "source": [
        "import pandas as pd # Data analysis and manipultion tool\n",
        "import numpy as np # Fundamental package for linear algebra and multidimensional arrays\n",
        "from PIL import Image as Im # For converting to numpy array\n",
        "#import tensorflow as tf # Deep Learning Tool\n",
        "import keras\n",
        "from keras import layers, Input\n",
        "import os # OS module in Python provides a way of using operating system dependent functionality\n",
        "import cv2 # Library for image processing\n",
        "from sklearn.model_selection import train_test_split # For splitting the data into train and validation set\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMbwcqWfz699"
      },
      "source": [
        "# Loading and preparing training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxrJ62rPztsh"
      },
      "source": [
        "labels = pd.read_csv(\"/content/content/eye_gender_data/Training_set.csv\") # loading the labels\n",
        "file_paths = [[fname, '/content/content/eye_gender_data/train/' + fname] for fname in labels['filename']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "zKE4f2kB00oo",
        "outputId": "26701a03-387a-4e2a-f43d-a4322431b2d9"
      },
      "source": [
        "images = pd.DataFrame(file_paths, columns=['filename', 'filepaths'])\n",
        "images.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>filepaths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Image_1.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Image_2.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Image_3.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image_4.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Image_5.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_5...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      filename                                          filepaths\n",
              "0  Image_1.jpg  /content/content/eye_gender_data/train/Image_1...\n",
              "1  Image_2.jpg  /content/content/eye_gender_data/train/Image_2...\n",
              "2  Image_3.jpg  /content/content/eye_gender_data/train/Image_3...\n",
              "3  Image_4.jpg  /content/content/eye_gender_data/train/Image_4...\n",
              "4  Image_5.jpg  /content/content/eye_gender_data/train/Image_5..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "3vHwcoHdFnpq",
        "outputId": "3d9fdc49-3937-4a6f-90d5-580722eafe85"
      },
      "source": [
        "train_data = pd.merge(images, labels, how = 'inner', on = 'filename')\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>filepaths</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Image_1.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_1...</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Image_2.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_2...</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Image_3.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_3...</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Image_4.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_4...</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Image_5.jpg</td>\n",
              "      <td>/content/content/eye_gender_data/train/Image_5...</td>\n",
              "      <td>male</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      filename                                          filepaths   label\n",
              "0  Image_1.jpg  /content/content/eye_gender_data/train/Image_1...    male\n",
              "1  Image_2.jpg  /content/content/eye_gender_data/train/Image_2...  female\n",
              "2  Image_3.jpg  /content/content/eye_gender_data/train/Image_3...  female\n",
              "3  Image_4.jpg  /content/content/eye_gender_data/train/Image_4...  female\n",
              "4  Image_5.jpg  /content/content/eye_gender_data/train/Image_5...    male"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "yfKYz4UrdyBf",
        "outputId": "2e1075a0-e455-4f5f-ff13-8e5b2435e5ad"
      },
      "source": [
        "Im.open(train_data.filepaths[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=58x58 at 0x7FA557689E90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "oAvfGZfFwbPd",
        "outputId": "f72c4241-18d6-49a4-88b1-d3435811b3e4"
      },
      "source": [
        "Im.open(train_data.filepaths[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADUAAAA1CAIAAABuhDQnAAAP8UlEQVR4nF2Zy28cVdPG+3L6PveLZzzGJgaFBIiEghIUCTZsYM2/i1iwQmKBhEAQAkoYOx7P2J5L36bvPf0ufuF81jcLa9zT3adO1VNPPVVH/eGHH1RVVVU1SRLDMFzXNU3zcDg0TaNpWl3Xh8NBUZSiKIqi0DRNVVVN03Rd59eyLJum0XXdMIw8z4UQaZo2TdM0TVVVVVUJIUzTLMuSe4qi4IVN09i2ned5XddN0xwOB5au67qqKrm0kN/4KIpS17WqqizQNA1Xqqo6HA51Xdu2zRsNw9B1Xe6kKIosywzDwDhFUdhMkiRZlvG9qirDMFRVretaCFGWpaZpnU5HVdUwDPM8T5JE13VFUVRVZSeiaZqyLLFACNE0TV3XLIMdiqJwAxvAE6yhqqqu6zyrqio7qeu6KIqmaYQQeFpRFNu25f4VRTkcDrquO47DroQQPG4YhqZp0llVVQl8y2O8i9+apsEIPMeedF3HYh4+HA5CCGkuMNA0zXEc7qnrmvhkWYbRqqpalmXbtqZp+/3+cDh4nkfoCUJRFPiFmwULY7VpmnJtzAVDAJQ7CZOqqkKIqqqyLKvrWtd1vJXnua7rLJ/nORswTbOqKsuymqaRyxuG0Wq1qqra7/dcPxwOlmWx1f1+n2VZVVUCC3C4qqo4D3OlL4Ezt2EoO5bXy7JUVbXdbtu23TRNkiQyb/C6RAIWG4ZRVdVisSjL0rZtVVVJFGzi33f4w6X7/b7X6+G2qqoAPrAjary6KArpRZbEPSAvy7IwDB3HIXWKooiiiESR4QPuPNtqtXBVHMdxHOu6PhgM2DPwPRwOAp8lSdLtdrGvLEvLsjRNS5IEb2GfqqplWbIelpEKqqqST4S4KIrb29uqqgaDQbvdjuM4CAJN00g43oxTdF03TRO/5nkeBMHV1dVut/M8bzQanZ6ettttUVUVXinL0jAMPJ/nOZaB7jzP0zTFUMMwgNf9cAM1gEX6W5YVhmGapq7rNk0D+HzfL4pCCKEoyn6/Byf7/d6yLOLAe/I8X61WhmEsFgvBwjKL+QsIiqKAbLMskyjM8xyKIXMlp5RlCT1FUZRlmW3biqLsdjt8dnV1Req4rttqtfI8xzhYOsuyu7u7LMva7fbp6elyubRt+/r62jRNEQQBtJ7nOSiR2CR/4zj2fR+vSIOgbplSXEmSRNM0IUQURXgF7G+3281mk6YpKa9p2ng8tm2bZGq326Ttbre7uLg4Ozs7Ojq6vLyMouj8/FwURVGWJZyy3+/BNZFK0zTLsjzP8S4UL2kcsAIA9gMSdF33PC/LsqurK8/zhBBBEDiO43keZUNRlCRJgiBIkmQ2m+Ea13WHw6FhGMvl0vf90Wjkuu5isRDwJBtN09Q0TR5ommaz2SiKkmUZIeYiniuKAhIhnQFikiSkGtyW5zlZSVHe7XawJnZ4nqdp2mKx0DTtgw8+sG07jmPHcfr9/uvXry8uLk5OTsbjsUiSxPf9fr8P2iAzimaSJHVdB0EggRXHcZIkVVXdL3dwkOM42+0Wyri7u5OVSlXVVqtF3oDpm5ubTqfjuq7neb1eL8/zy8vLdrs9Go0cx3n16tVgMHj8+PHff/99cXEhNptNFEWapmmalqYpyR8EAXBcrVY3Nzeu67bbbZAQhiHFSpZImAUREAQBCUHQW61WWZbL5dJ1XRJzOBy6rpvneRRFruseHx97nmea5mazubq6evTo0cOHD+fz+Zs3b6bT6W63E/v9vizLNE0ty4K+syxbr9cQWBRFEG+apkEQ4F0StvnvQ3x933ddl9SGs3Rdz7KMqg8S0jRdrVbD4VDS1tu3b2ezmRBiNBp99NFHFxcXiqKcn59fX1+vVivHcQTkGYYhHLHb7dI0hYdubm6SJKHkA0HyFNKRVYSLuq6jlzAXstzv91VVua7LnXgRQNd13el0kiS5vb0Nw9B13clkUlVVt9u9ublJ03Q4HHa7XWHbNnlKiLGyqirf92FgwooduAdxipCBFDudjm3b5JNpmlhPeSjLErPYJ/hB1wBNwzB6vZ5pmrBslmVwCC4ThmGkaer7/m63g82xD4vh/SRJpNyS7iEZ+WJZluu6KE2uSzlD8YAdqUywhKqqV1dXhmH0+/1WqwU24HzTNFutlud53W5XIHpd1727u0vTlMSkwsKIVFhSWOol0zRt26Z04oz9fk/OwuGKogghPM8je2A+vMhFHM8qZVnGccxPks7gSEEa2rbdarUg9CiKcBIkB9IRRa1Wq91uS4mLSkMO5Xm+Xq+lcVjvOI5pmr1eD0IhV5DNJKVhGLQ7pmkiXhzHwZdRFJVlKWA4dLYsxJZlIRsRw5ZlmaaJP6hvYRj6vh+GIdBM01RKV7Q3zLfdbnVdn8/nhmG02+1Wq2WapmVZCIJOpwMw6Gn4UKJs2+73+7quCxYARnAHIEjTtCxLoIBgwWdIoyiKNpsNN1dVhQTsdDryQcodggiBHUURtTHLsk6n0+v1ID8ZMYx2HCdNUzaZZZlAQkIQVLk4jpfLJYp8PB7zAJyiadpkMkmShF3WdW2apqZpw+Hw+fPneZ4PBoM///xzsVjYtn17e0uiSKc+ePBgOp3+9ttv6/V6uVyu12vLsgaDwenp6XA4tCyr0+mEYYj1REkg+2Tv5Pt+HMemaXY6neFwmKYpArOqqqOjo36/PxqNmqa5vb2Fz8CloiiXl5dv3rwxDGO320E3x8fHsMFkMoF63nvvvRcvXjx9+nS1Wr18+fKPP/7wfR9bx+PxZDLh5WQb0BLtdjsIAuCFKbyo1WpRr5BlZMZkMnFdlwIK9YAzy7Jev34Ng0hmxjemaU6nU03T1us1Uqqqqk8++eTRo0ej0ejy8nK1WgVBEIbhcrmcTqfj8Xg6nQohXNc1DEOoquo4DoRcFEW32x0MBoZhRFG0Wq3SNIU8u93ueDz2PM+yrNFolOf5X3/9RRtmWdbZ2dn5+flPP/2Et87OzuBnqoKqqv/++y+AsSwrCIKyLI+Ojj7//HPXdR3Hmc/naKL9fo+Knk6nlmUdDgeBlIU52+32YDCwbXu5XL58+bKqqidPnsxmszAMLctqt9vkmm3bVAXf9/M83+/3w+Hw5OTEsixFUfr9/ng8hj6iKOL9SZJMJpOTk5Ner5dlmeM4dDmTyQQ+CsNwPp+naep5Ht0FxCRc10VBDQaD2WxmGMbd3V3TNKPRaLlcPnz48Pz8/M2bN5AObWXTNMfHxw8fPvz1119J/N1ul2VZq9WCp96+fStnErqur9fruq6fP3/+4Ycfwsmo/ziOeQQB6vv+4XAIgmAymWy3W2qB5jgOD8xmM8dxrq+vwzA8Pz9/9uxZq9VKkmS5XG63W4Q7gsV13W63+/Tp05OTk7qu4QjKa7fbRbbs93tEpO/7ZO5nn302m836/f5sNiOBXNe1LOv4+JjKNh6PaS3QwpvNZrFYCNd1j46OLMvq9/ubzeb6+tqyLM/zVFV99uxZlmW///47PZRpmp7nDYdDxiWffvppHMf7/d73fV3Xu91uXdeQEb2PEILFOp3O119//eLFiyRJ4jju9/tI2n6/L3tZXddns1kcx3Vdx3HcbreFEGEYCl5tWZbv+77vdzqd5XL5448/ttvtbrebpikbZUld1y8vL03TfO+99xzH+eKLL0zT/P77729vb+M4tiwrSRI5o4jjOMuyo6Oj77777ssvv4TIEOqGYVCuaIWEEJPJBAkzn8+RCJgkUOToTU3TMLwoCtu2oyiitaPbffXqleM4nU6nLMuiKEaj0XQ6/eabb05OTn7++ef5fL5er8uyRF0LIY6Ojh4/fvzVV189efLENM00TUkLOkN62cVicXFxAUMZhnF8fOw4zj///NM0Ta/XGw6HwrIs2g6KtG3bctDRbrebptlut1ReRCuzqd1uF4bhdrt9//33J5PJt99+u16v5/P5zc1NWZbdbpcBwGw2Oz09PRwOcRzTO+u6jmOiKIqiaLvdIpCRc6ZpHh8fCyHm83kYhrPZTOAMKintICoB+zRN8zyPXoau9ujoiBkKEuOXX37pdrudTscwjOfPn9NcWpaFFMiyLIoiRACeOxwOy+VytVqROlVVOY4zGAxk21WW5ePHj1VVjaKoKApBAUAOUSXlmIL6SHrGcYzzoFlVVcfjsaIokFFVVavV6vr6utvtOo7T6/VQaJ1OpyiKTqeDMAnDEKGKRAXQcRwjugaDgfTCxx9/vN/vLy4uBPchT1CadDd4Gz0CYdq2zZAZlsrzvNfrUbKQ3JPJxPd9RC5q9Pb2tmkaz/Oo+rqub7dbSgucgkfQcpvNhhgyzmq1WrPZTMixKQsjYcAfU2hSCU7idRSbKIpoXMjuJElgNYYejBZggKqq6MbROwjmu7s7nG0YBpHZ7Xas1el0aOaPj48FDSymEH7aCxRDkiTb7ZZKL2fziqJ4nud5Hi5HEaqqend35zgOXT0NGOGrqoqKt91uGbhgPYyBuAcVyOkkSfr9Pr2ShjNkh4ZkB4tBEBwOB0pCEASMi2lWSO2qqlqt1mg0GgwGTDaY0F9cXOi6vlwuy7K8vb3Vdf3t27cMFRaLRRAEp6enQojlclkUBeq/qioIxbbt6XQK6Y5GI8FUSiol8h+dzNCdTgdZj94Er0IIoqlpmqIo0+lUURSGnASd1zLTYPLEEGO322232wcPHgwGA6J3fHzMbcgqKcjfzcdlzwb4aFFZFfVF0kgkUN/YCXlKNwmDCiGm06lhGJQlyjqNWbvdNk2TIenZ2ZnjOGEYonNl40de0hXQ+wkiS7xkhyuJEIsNw+AK32VbCbqR39zJDICRGaobWZkkiRCi3+9TS5qmcRxHHnvQJkOrrEU+CTnVl0DECDYkx6b0GeCPjaJl5OGCDD3dVxRFVEJN0+I4xkOoAdoGJI/krPtHS6QguxVy2o8RtGpy3MugF2iCCVp/rJGgxHQ6ZQiLKQcLAH9VVaF6hiQoHaYcEjNAH8twk6C3lWcm3Id97zoUIRg8EgJien8sxBSBv3IMwviVFIGHCQj6FN/TWOV5fv90jpvxn6Io747IiBqOZLbHJqhvQghKCLqNeQVIkEoEmyQVcBEMyfMmOSpB/TM5kA/K2QPOA0KCeOEeeU7HhuQ8GUKmgedhwMAC7FuOpO6fh3FsgaC6fz5F6PkOilhCulmOIoTMXPpf3CNZBqjSSkIERVHwOpkQJN27cAjBdAybsB4vyg38Pw/xfmItKUIiRzT/HdXR/NEBULzloY98EjvkMnIwwM1yYim/y66KcBN6bJIkT8TxmRzh/V98cQbBYhMQgdylRKtcW+aQeu9QU9553z3KvTEh0eQnrJFmsZ/7tFL/d0YuKB4SmxQASd/qf0evbIM04klpKKwudyzzlO/31+ZfyU0yRJJNVVWVMHs3buQZFr6fjJglqwXnpWxaAhx/cz+2ytDI7cm0UO4dLHIzPCUzRnLT4b/D78Ph8D+lmHQJbkFspwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=53x53 at 0x7FA557224110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAvWNh7nSzPc",
        "outputId": "988a5883-a5cc-45c8-8656-709d1be6e595"
      },
      "source": [
        "data = [] # initialize an empty numpy array\n",
        "image_size = 100 # image size taken is 100 here. one can take other size too\n",
        "for i in range(len(train_data)):\n",
        "\n",
        "  img_array = cv2.imread(train_data['filepaths'][i], cv2.IMREAD_GRAYSCALE) # converting the image to gray scale\n",
        "\n",
        "  new_img_array = cv2.resize(img_array, (image_size, image_size)) # resizing the image array\n",
        "  data.append([new_img_array, train_data['label'][i]])\n",
        "print(data[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[array([[188, 188, 189, ..., 176, 175, 175],\n",
            "       [189, 189, 188, ..., 174, 173, 172],\n",
            "       [190, 189, 188, ..., 168, 167, 167],\n",
            "       ...,\n",
            "       [133, 137, 144, ..., 168, 167, 166],\n",
            "       [134, 138, 145, ..., 165, 164, 163],\n",
            "       [135, 139, 146, ..., 163, 162, 162]], dtype=uint8), 'male'], [array([[167, 169, 173, ..., 194, 195, 195],\n",
            "       [168, 170, 173, ..., 193, 194, 195],\n",
            "       [171, 171, 173, ..., 192, 193, 194],\n",
            "       ...,\n",
            "       [183, 185, 189, ..., 199, 197, 196],\n",
            "       [183, 186, 189, ..., 199, 197, 195],\n",
            "       [184, 186, 190, ..., 199, 196, 195]], dtype=uint8), 'female']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNmHduVcFntO",
        "outputId": "5ad05c22-bafb-4811-8eaf-cc1d76ec4f86"
      },
      "source": [
        "image_copy = np.copy(data)\n",
        "size_x, size_y = image_copy.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:792: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, order=order, subok=subok, copy=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y4X1Dj2VLDB",
        "outputId": "b31a31a0-4781-47a9-fc64-934f3c09d1b9"
      },
      "source": [
        "size_x, size_y\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9220, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xt9fqVMU0-iv"
      },
      "source": [
        "# Data Pre-processing\n",
        "It is necessary to bring all the images in the same shape and size, also convert them to their pixel values because all machine learning or deep learning models accepts only the numerical data. Also we need to convert all the labels from categorical to numerical values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSTW1qCS1K2O"
      },
      "source": [
        "def import_data(labels_ = \"/content/content/eye_gender_data/Training_set.csv\",\n",
        "                files_ = '/content/content/eye_gender_data/train/',\n",
        "                train_data_ =True):\n",
        "  labels = pd.read_csv(labels_) # loading the labels\n",
        "  file_paths = [[fname, files_ + fname] for fname in labels['filename']]\n",
        "  train_data = pd.merge(images, labels, how = 'inner', on = 'filename')\n",
        "  data = [] # initialize an empty numpy array\n",
        "  image_size = 100 # image size taken is 100 here. one can take other size too\n",
        "  for i in range(len(train_data)):\n",
        "\n",
        "    img_array = cv2.imread(train_data['filepaths'][i], cv2.IMREAD_GRAYSCALE) # converting the image to gray scale\n",
        "\n",
        "    new_img_array = cv2.resize(img_array, (image_size, image_size)) # resizing the image array\n",
        "    if train_data_ == True:\n",
        "      data.append([new_img_array, train_data['label'][i]])\n",
        "    else:\n",
        "      data.append([new_img_array])\n",
        "    X = np.squeeze(data)\n",
        "  return X\n",
        "\n",
        "\n",
        "def preprocess_img(X, train_data_ = True, proportion=0.2, random_state=None):\n",
        "  n = len(X)\n",
        "  for i in range(len(X)):\n",
        "   X[i][0] = X[i][0]/255.\n",
        "  \n",
        "  if train_data_ == True :\n",
        "    x_train = np.array([X[i][0] for i in range(len(X))]).reshape((n,100,100,1))\n",
        "    y_train = [X[i][1] for i in range(len(X))]\n",
        "    y_train_coded = np.array([1 if i=='male' else 0 for i in y_train])\n",
        "    y_train_coded.reshape(n,1)\n",
        "    return train_test_split(x_train, y_train_coded,\n",
        "                            test_size=proportion,\n",
        "                            random_state=random_state)\n",
        "  else:\n",
        "    x_test = np.array([X[i] for i in range(len(X))]).reshape((n,100,100,1))\n",
        "    return x_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muA_CHlYl_yJ",
        "outputId": "ee8455a5-578e-4a51-c2e3-8b878d9b2599"
      },
      "source": [
        "X = import_data()\n",
        "x_train,  x_validate, y_train, y_validate = preprocess_img(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moUm_8G51LWf"
      },
      "source": [
        "# Building Model & Hyperparameter tuning\n",
        "Now we are finally ready, and we can train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9to2L7Q1Mjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07fdb82-4c09-430b-cc39-3f10f28acd0b"
      },
      "source": [
        "# define input shape\n",
        "INPUT_SHAPE = (100, 100, 1)\n",
        "\n",
        "# define sequential model\n",
        "model = keras.models.Sequential()\n",
        "# define conv-pool layers - set 1\n",
        "model.add(layers.Conv2D(filters=10, kernel_size=(3, 3), strides=(1, 1),\n",
        "activation='relu', padding='valid', input_shape=INPUT_SHAPE))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\"))\n",
        "model.add(layers.Conv2D(filters=10, kernel_size=(2, 2), strides=(1, 1),\n",
        "activation='relu', padding='valid', input_shape=INPUT_SHAPE))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\"))\n",
        "\n",
        "\n",
        "# add flatten layer\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# add dense layers with some dropout\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(rate=0.2))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(rate=0.3))\n",
        "\n",
        "\n",
        "# add output layer\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compile model\n",
        "model.compile(optimizer='adam',\n",
        "loss='BinaryCrossentropy',\n",
        "metrics=['accuracy','AUC'])\n",
        "\n",
        "# view model layers\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 98, 98, 10)        100       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 49, 49, 10)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 48, 48, 10)        410       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 10)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 5760)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               1474816   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 1,541,375\n",
            "Trainable params: 1,541,375\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Htp-iX1UEO"
      },
      "source": [
        "# Validate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awbHjw6N1Xfr",
        "outputId": "8941c3be-933c-4943-9041-fee9fac4e667"
      },
      "source": [
        "model.fit(x_train, y_train, batch_size=32, epochs=15, verbose=1, \n",
        "        shuffle=True, validation_data = (x_validate,y_validate),\n",
        "        use_multiprocessing=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "231/231 [==============================] - 41s 159ms/step - loss: 0.6285 - accuracy: 0.6316 - auc: 0.6689 - val_loss: 0.4180 - val_accuracy: 0.8102 - val_auc: 0.8954\n",
            "Epoch 2/15\n",
            "231/231 [==============================] - 35s 149ms/step - loss: 0.4229 - accuracy: 0.8141 - auc: 0.8856 - val_loss: 0.3186 - val_accuracy: 0.8644 - val_auc: 0.9399\n",
            "Epoch 3/15\n",
            "231/231 [==============================] - 35s 151ms/step - loss: 0.3340 - accuracy: 0.8621 - auc: 0.9296 - val_loss: 0.2862 - val_accuracy: 0.8834 - val_auc: 0.9511\n",
            "Epoch 4/15\n",
            "231/231 [==============================] - 35s 150ms/step - loss: 0.2963 - accuracy: 0.8783 - auc: 0.9450 - val_loss: 0.2700 - val_accuracy: 0.8850 - val_auc: 0.9576\n",
            "Epoch 5/15\n",
            "231/231 [==============================] - 35s 151ms/step - loss: 0.2673 - accuracy: 0.8853 - auc: 0.9558 - val_loss: 0.2376 - val_accuracy: 0.9062 - val_auc: 0.9649\n",
            "Epoch 6/15\n",
            "231/231 [==============================] - 35s 151ms/step - loss: 0.2266 - accuracy: 0.9077 - auc: 0.9679 - val_loss: 0.2529 - val_accuracy: 0.8905 - val_auc: 0.9664\n",
            "Epoch 7/15\n",
            "231/231 [==============================] - 35s 152ms/step - loss: 0.2121 - accuracy: 0.9091 - auc: 0.9724 - val_loss: 0.2900 - val_accuracy: 0.8774 - val_auc: 0.9652\n",
            "Epoch 8/15\n",
            "231/231 [==============================] - 35s 153ms/step - loss: 0.1952 - accuracy: 0.9211 - auc: 0.9762 - val_loss: 0.2167 - val_accuracy: 0.9111 - val_auc: 0.9718\n",
            "Epoch 9/15\n",
            "231/231 [==============================] - 35s 154ms/step - loss: 0.1474 - accuracy: 0.9428 - auc: 0.9865 - val_loss: 0.2329 - val_accuracy: 0.9094 - val_auc: 0.9683\n",
            "Epoch 10/15\n",
            "231/231 [==============================] - 35s 152ms/step - loss: 0.1215 - accuracy: 0.9545 - auc: 0.9909 - val_loss: 0.2369 - val_accuracy: 0.9170 - val_auc: 0.9697\n",
            "Epoch 11/15\n",
            "231/231 [==============================] - 35s 152ms/step - loss: 0.1170 - accuracy: 0.9523 - auc: 0.9915 - val_loss: 0.2354 - val_accuracy: 0.9100 - val_auc: 0.9676\n",
            "Epoch 12/15\n",
            "231/231 [==============================] - 35s 152ms/step - loss: 0.1024 - accuracy: 0.9634 - auc: 0.9933 - val_loss: 0.2763 - val_accuracy: 0.8980 - val_auc: 0.9633\n",
            "Epoch 13/15\n",
            "231/231 [==============================] - 35s 150ms/step - loss: 0.0754 - accuracy: 0.9683 - auc: 0.9967 - val_loss: 0.2983 - val_accuracy: 0.9116 - val_auc: 0.9647\n",
            "Epoch 14/15\n",
            "231/231 [==============================] - 35s 151ms/step - loss: 0.0655 - accuracy: 0.9741 - auc: 0.9971 - val_loss: 0.3161 - val_accuracy: 0.8970 - val_auc: 0.9656\n",
            "Epoch 15/15\n",
            "231/231 [==============================] - 35s 152ms/step - loss: 0.0592 - accuracy: 0.9793 - auc: 0.9976 - val_loss: 0.3407 - val_accuracy: 0.9056 - val_auc: 0.9590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa55b209cd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBdZ87eF1ZA3"
      },
      "source": [
        "# Predict The Output For Testing Dataset ðŸ˜…\n",
        "We have trained our model, evaluated it and now finally we will predict the output/target for the testing data (i.e. Test.csv).\n",
        "\n",
        "Load Test Set\n",
        "Load the test data on which final submission is to be made.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5ImO3X21aZR"
      },
      "source": [
        "X_test = import_data(labels_=\"/content/content/eye_gender_data/Testing_set.csv\", \n",
        "                     files_='/content/content/eye_gender_data/test/', train_data_=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzhlWiLT7sik",
        "outputId": "ea71b676-18d4-4cb5-f9b4-fe7ab08de03c"
      },
      "source": [
        "X_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0,   0,   0],\n",
              "       [178, 180, 178, ...,  92, 108, 121],\n",
              "       [177, 177, 175, ...,  69,  85,  98],\n",
              "       ...,\n",
              "       [153, 156, 158, ..., 188, 189, 189],\n",
              "       [152, 154, 157, ..., 188, 189, 190],\n",
              "       [150, 153, 156, ..., 189, 190, 191]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA3nWmOe1gFN"
      },
      "source": [
        "# Data Pre-processing on test_data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ac4WNBl1hJP"
      },
      "source": [
        "x_test = preprocess_img(X_test, train_data_=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF1d-o1I1m97"
      },
      "source": [
        "# Make Prediction on Test Dataset\n",
        "Time to make a submission!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd_iyghFnJ-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f2bcb0f-4e2c-4b9f-e050-e6a1c62748e8"
      },
      "source": [
        "prediction_code = model.predict(x_test)\n",
        "predictions = np.array(['male' if i==1 else 'female' for i in prediction_code])\n",
        "predictions[1:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['female', 'male', 'female', 'male'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQfU8KB12ERk"
      },
      "source": [
        "# save prediction results locally via colab notebook\n",
        " A file named 'prediction_results' will be downloaded in my system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ8lPqHo2ID9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "711ea062-80cb-41a0-bd94-151b3ddfc25e"
      },
      "source": [
        "# To create Dataframe of predicted value with particular respective index\n",
        "res = pd.DataFrame(predictions) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\n",
        "res.columns = [\"label\"]\n",
        "\n",
        "# To download the csv file locally\n",
        "# from google.colab import files\n",
        "res.to_csv('prediction_results.csv', index = False)         \n",
        "files.download('prediction_results.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9be5346d-a12a-4d06-9281-21f9f0b71e8d\", \"prediction_results.csv\", 13817)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NId-T43AU9a"
      },
      "source": [
        "# conclusion\n",
        "It was amazing journey with deep learning. I learnt lot about deep learning on image dataset to determine gender using eye detector. I have run few epochs for validation and found good result. "
      ]
    }
  ]
}